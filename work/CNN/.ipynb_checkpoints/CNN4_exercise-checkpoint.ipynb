{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae67483-6500-4d34-9031-8424ca4c803b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MinMaxScaler, keine One Hot Input Vectors, kein Sigmoid, BCELossWithLogitLoss (with weight), Cross Validation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import time\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "torch.manual_seed(42) # Reproduzierbarkeit des Shuffles im DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f158ae10-e0c4-41c8-89d0-df47927aa4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load tha data Goal: Load the dataset from a CSV and extract features and labels.\n",
    "# 1.1 Define a data_path string with the full path to your dataset (CSV format).\n",
    "# 1.2 Load the dataset using pandas.read_csv() with appropriate separator and decimal format.\n",
    "# 1.3 Extract features X (all columns except the first) and labels y (first column).\n",
    "# 1.4 Separate static and sequential features manually:\n",
    "# Static: max_force, avg_force, avg_speed → columns 0, 1, 7.\n",
    "# Sequential: extract 3 sequences (Force, Speed, Distance) each of 5 values.\n",
    "# 1.5 Stack sequences using np.stack to shape as (samples, 3, 5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c243b7-2313-41c3-adda-bf630ec8d851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. CNN Model Define a 1D CNN model in PyTorch that processes both sequential and static input data.\n",
    "# 2.1 Create a class CNNModel that inherits from nn.Module.\n",
    "# 2.2 In __init__:\n",
    "# Define Conv1d with in_channels=3, out_channels=16, and kernel_size=3, padding=1.\n",
    "# Add MaxPool1d to reduce dimensionality.\n",
    "# Define a fully connected layer fc1 that takes 16×3 + 3 inputs (3 pooled timesteps × 16 channels + 3 static features).\n",
    "# Define fc2 to produce a single output (binary classification).\n",
    "# 2.3 In forward:\n",
    "# Apply ReLU after convolution.\n",
    "# Apply pooling.\n",
    "# Flatten sequence features and concatenate with static features.\n",
    "# Pass through fc1 and fc2.\n",
    "# Use squeeze(1) to get a 1D output.\n",
    "# 2.4 Do NOT use Sigmoid in the model — use BCEWithLogitsLoss later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e30b57-f4ab-449a-b2ee-7d4323b1c011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 Cross-Validation Setup Set up 5-fold stratified cross-validation and initialize result containers.\n",
    "# 3.1 Use StratifiedKFold from sklearn with n_splits=5, shuffle=True, random_state=42.\n",
    "# 3.2 Create lists to store:\n",
    "# all_test_probabilities → predicted probabilities.\n",
    "# misclassified_samples → details of incorrect predictions.\n",
    "# total_conf_matrix → accumulate confusion matrices.\n",
    "# all_training_losses → per-epoch training loss for each fold.\n",
    "# test_losses → test loss per fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1dae10-4e9b-469a-812f-b3ed0342c08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  4. Fold Loop - Training and Evaluation Goal: For each fold, preprocess, train, and evaluate.\n",
    "# 4.1 Split Data: Use current train/test indices to split sequence_features, static_features, and y.\n",
    "# 4.2 MinMax Normalization:\n",
    "# Flatten sequence features (reshape to 2D) → required by MinMaxScaler.\n",
    "# Use separate MinMaxScaler for sequence and static features.\n",
    "# Fit on training data, transform both training and test sets.\n",
    "# Reshape sequence back to (samples, 3, 5).\n",
    "# 4.3 Convert to Tensors:\n",
    "# Use torch.tensor() to convert all inputs and targets (float32).\n",
    "# Ensure targets are of shape (batch,) — not one-hot encoded.\n",
    "# 4.4 TensorDataset & DataLoader:\n",
    "# Use TensorDataset to combine X_seq, X_static, and y.\n",
    "# Use DataLoader with batch_size=40 and shuffle=True.\n",
    "# 4.5 Class Imbalance Handling:\n",
    "# Count class samples: sum(y_train == 0) and sum(y_train == 1).\n",
    "# Compute pos_weight = class_0 / class_1 (used for BCEWithLogitsLoss).\n",
    "# 4.6 Model, Loss, Optimizer:\n",
    "# Instantiate CNNModel().\n",
    "# Use BCEWithLogitsLoss(pos_weight=...) — don’t use Sigmoid manually!\n",
    "# Use Adam optimizer with learning rate 0.001.\n",
    "# 4.7 Training Loop:\n",
    "# Run for 100 epochs.\n",
    "# For each batch:\n",
    "# Set model to train mode.\n",
    "# Compute output.\n",
    "# Compute loss using criterion.\n",
    "# Backpropagate with loss.backward().\n",
    "# Update weights using optimizer.step().\n",
    "# Track loss per epoch.\n",
    "# Save all epoch losses in a list.\n",
    "# 4.8 Evaluation on Test Set:\n",
    "# Set model to eval mode.\n",
    "# No gradient tracking (with torch.no_grad()).\n",
    "# Compute model outputs and apply torch.sigmoid() externally.\n",
    "# Predict labels as >= 0.5.\n",
    "# Compute test loss and confusion matrix.\n",
    "# Store test results and probabilities.\n",
    "# 4.9 Misclassified Samples:\n",
    "# Compare predictions and labels.\n",
    "# Store index, true label, predicted label, and type (FP or FN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04158581-d41e-46f5-8586-693563469f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Post-Processing and Results Goal: Summarize results and visualize outcomes.\n",
    "# 5.1 Sort Predicted Probabilities:\n",
    "# Sort all_test_probabilities by original index.\n",
    "# Extract and print probabilities per sample.\n",
    "# 5.2 Print Misclassified Samples:\n",
    "# Loop over list and print index, true, predicted, error type.\n",
    "# 5.3 Calculate Final Metrics:\n",
    "# From total_conf_matrix, extract TP, TN, FP, FN.\n",
    "# Compute:\n",
    "# Accuracy: (TP + TN) / total.\n",
    "# Precision: TP / (TP + FP).\n",
    "# Recall: TP / (TP + FN).\n",
    "# F1 Score: harmonic mean.\n",
    "# Average test loss: mean of test_losses.\n",
    "# Total time with time.perf_counter().\n",
    "# 5.4 Visualize Confusion Matrix:\n",
    "# Plot using sns.heatmap.\n",
    "# Format labels (Positive/Negative).\n",
    "# 5.5 Plot Training Loss:\n",
    "# For each fold, plot epoch-wise loss curve.\n",
    "# Label axes and include a legend.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815e95dd-fde5-443e-9344-8c5a71df1932",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
